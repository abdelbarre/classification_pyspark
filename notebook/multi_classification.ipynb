{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In This notebook we will **explore** and **classify San Francisco Crime Description into 33 pre-defined categories**. \n",
    "\n",
    "The data can be downloaded from [Kaggle](https://www.kaggle.com/c/sf-crime/data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data :\n",
    "\n",
    "Given a new crime description comes in, we want to assign it to one of 33 categories. The classifier makes the assumption that each new crime description is assigned to one and only one category. This is multi-class text classification problem.\n",
    "    \n",
    "    ``Input: Descript``\n",
    "\n",
    "Example: “ STOLEN AUTOMOBILE”\n",
    "\n",
    "    ``Output: Category``\n",
    "\n",
    "Example: VEHICLE THEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "# sc =SparkContext()\n",
    "# sqlContext = SQLContext(sc)\n",
    "data = spark.read.csv('../data/train.csv',header=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Dates: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Descript: string (nullable = true)\n",
      " |-- DayOfWeek: string (nullable = true)\n",
      " |-- PdDistrict: string (nullable = true)\n",
      " |-- Resolution: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- X: string (nullable = true)\n",
      " |-- Y: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the structure of columns \n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "That’s it! We have loaded the dataset. Let’s start exploring.\n",
    "Remove the columns we do not need and have a look the first five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-------------------+------------------+\n",
      "|              Dates|      Category|            Descript|DayOfWeek|PdDistrict|    Resolution|             Address|                  X|                 Y|\n",
      "+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-------------------+------------------+\n",
      "|2015-05-13 23:53:00|      WARRANTS|      WARRANT ARREST|Wednesday|  NORTHERN|ARREST, BOOKED|  OAK ST / LAGUNA ST|  -122.425891675136|  37.7745985956747|\n",
      "|2015-05-13 23:53:00|OTHER OFFENSES|TRAFFIC VIOLATION...|Wednesday|  NORTHERN|ARREST, BOOKED|  OAK ST / LAGUNA ST|  -122.425891675136|  37.7745985956747|\n",
      "|2015-05-13 23:33:00|OTHER OFFENSES|TRAFFIC VIOLATION...|Wednesday|  NORTHERN|ARREST, BOOKED|VANNESS AV / GREE...|   -122.42436302145|  37.8004143219856|\n",
      "|2015-05-13 23:30:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|  NORTHERN|          NONE|1500 Block of LOM...|-122.42699532676599| 37.80087263276921|\n",
      "|2015-05-13 23:30:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|      PARK|          NONE|100 Block of BROD...|  -122.438737622757|37.771541172057795|\n",
      "|2015-05-13 23:30:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday| INGLESIDE|          NONE| 0 Block of TEDDY AV|-122.40325236121201|   37.713430704116|\n",
      "|2015-05-13 23:30:00| VEHICLE THEFT|   STOLEN AUTOMOBILE|Wednesday| INGLESIDE|          NONE| AVALON AV / PERU AV|  -122.423326976668|  37.7251380403778|\n",
      "|2015-05-13 23:30:00| VEHICLE THEFT|   STOLEN AUTOMOBILE|Wednesday|   BAYVIEW|          NONE|KIRKWOOD AV / DON...|  -122.371274317441|  37.7275640719518|\n",
      "|2015-05-13 23:00:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|  RICHMOND|          NONE|600 Block of 47TH AV|  -122.508194031117|37.776601260681204|\n",
      "|2015-05-13 23:00:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|   CENTRAL|          NONE|JEFFERSON ST / LE...|  -122.419087676747|  37.8078015516515|\n",
      "|2015-05-13 22:58:00| LARCENY/THEFT|PETTY THEFT FROM ...|Wednesday|   CENTRAL|          NONE|JEFFERSON ST / LE...|  -122.419087676747|  37.8078015516515|\n",
      "|2015-05-13 22:30:00|OTHER OFFENSES|MISCELLANEOUS INV...|Wednesday|   TARAVAL|          NONE|0 Block of ESCOLT...|  -122.487983072777|37.737666654332706|\n",
      "|2015-05-13 22:30:00|     VANDALISM|MALICIOUS MISCHIE...|Wednesday|TENDERLOIN|          NONE|  TURK ST / JONES ST|-122.41241426358101|  37.7830037964534|\n",
      "|2015-05-13 22:06:00| LARCENY/THEFT|GRAND THEFT FROM ...|Wednesday|  NORTHERN|          NONE|FILLMORE ST / GEA...|  -122.432914603494|  37.7843533426568|\n",
      "|2015-05-13 22:00:00|  NON-CRIMINAL|      FOUND PROPERTY|Wednesday|   BAYVIEW|          NONE|200 Block of WILL...|  -122.397744427103|  37.7299346936044|\n",
      "|2015-05-13 22:00:00|  NON-CRIMINAL|      FOUND PROPERTY|Wednesday|   BAYVIEW|          NONE|0 Block of MENDEL...|-122.38369150395901|  37.7431890419965|\n",
      "|2015-05-13 22:00:00|       ROBBERY|ROBBERY, ARMED WI...|Wednesday|TENDERLOIN|          NONE|  EDDY ST / JONES ST|  -122.412597377187|37.783932027727296|\n",
      "|2015-05-13 21:55:00|       ASSAULT|AGGRAVATED ASSAUL...|Wednesday| INGLESIDE|          NONE|GODEUS ST / MISSI...|  -122.421681531572|  37.7428222004845|\n",
      "|2015-05-13 21:40:00|OTHER OFFENSES|   TRAFFIC VIOLATION|Wednesday|   BAYVIEW|ARREST, BOOKED|MENDELL ST / HUDS...|-122.38640086995301|   37.738983491072|\n",
      "|2015-05-13 21:30:00|  NON-CRIMINAL|      FOUND PROPERTY|Wednesday|TENDERLOIN|          NONE|100 Block of JONE...|  -122.412249767634|   37.782556330202|\n",
      "+-------------------+--------------+--------------------+---------+----------+--------------+--------------------+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show data Simple \n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on both columns that we really need \" Category\" and \"Descript\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|      Category|            Descript|\n",
      "+--------------+--------------------+\n",
      "|      WARRANTS|      WARRANT ARREST|\n",
      "|OTHER OFFENSES|TRAFFIC VIOLATION...|\n",
      "|OTHER OFFENSES|TRAFFIC VIOLATION...|\n",
      "| LARCENY/THEFT|GRAND THEFT FROM ...|\n",
      "| LARCENY/THEFT|GRAND THEFT FROM ...|\n",
      "+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drop_list = ['Dates', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n",
    "data = data.select([column for column in data.columns if column not in drop_list])\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 20 crime categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            Category| count|\n",
      "+--------------------+------+\n",
      "|       LARCENY/THEFT|174900|\n",
      "|      OTHER OFFENSES|126182|\n",
      "|        NON-CRIMINAL| 92304|\n",
      "|             ASSAULT| 76876|\n",
      "|       DRUG/NARCOTIC| 53971|\n",
      "|       VEHICLE THEFT| 53781|\n",
      "|           VANDALISM| 44725|\n",
      "|            WARRANTS| 42214|\n",
      "|            BURGLARY| 36755|\n",
      "|      SUSPICIOUS OCC| 31414|\n",
      "|      MISSING PERSON| 25989|\n",
      "|             ROBBERY| 23000|\n",
      "|               FRAUD| 16679|\n",
      "|FORGERY/COUNTERFE...| 10609|\n",
      "|     SECONDARY CODES|  9985|\n",
      "|         WEAPON LAWS|  8555|\n",
      "|        PROSTITUTION|  7484|\n",
      "|            TRESPASS|  7326|\n",
      "|     STOLEN PROPERTY|  4540|\n",
      "|SEX OFFENSES FORC...|  4388|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "data.groupBy(\"Category\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 20 crime descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            Descript|count|\n",
      "+--------------------+-----+\n",
      "|GRAND THEFT FROM ...|60022|\n",
      "|       LOST PROPERTY|31729|\n",
      "|             BATTERY|27441|\n",
      "|   STOLEN AUTOMOBILE|26897|\n",
      "|DRIVERS LICENSE, ...|26839|\n",
      "|      WARRANT ARREST|23754|\n",
      "|SUSPICIOUS OCCURR...|21891|\n",
      "|AIDED CASE, MENTA...|21497|\n",
      "|PETTY THEFT FROM ...|19771|\n",
      "|MALICIOUS MISCHIE...|17789|\n",
      "|   TRAFFIC VIOLATION|16471|\n",
      "|PETTY THEFT OF PR...|16196|\n",
      "|MALICIOUS MISCHIE...|15957|\n",
      "|THREATS AGAINST LIFE|14716|\n",
      "|      FOUND PROPERTY|12146|\n",
      "|ENROUTE TO OUTSID...|11470|\n",
      "|GRAND THEFT OF PR...|11010|\n",
      "|POSSESSION OF NAR...|10050|\n",
      "|PETTY THEFT FROM ...|10029|\n",
      "|PETTY THEFT SHOPL...| 9571|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy(\"Descript\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline\n",
    "\n",
    "Spark Machine Learning Pipelines API is similar to Scikit-Learn. Our pipeline includes three pre-processing steps:\n",
    "\n",
    "  - 1  **regexTokenizer**: Tokenization (with Regular Expression)\n",
    "  \n",
    "  - 2  **stopwordsRemover**: Remove Stop Words\n",
    "  \n",
    "  - 3  **countVectors**: Count vectors (“document-term vectors”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## libs import\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression# regular expression tokenizer\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"Descript\", outputCol=\"words\", pattern=\"\\\\W\")# stop words\n",
    "\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] # You can add others stop words if you want\n",
    "\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)# bag of words count\n",
    "\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StringIndexer\n",
    "\n",
    "StringIndexer encodes a string column of labels to a column of label indices. \n",
    "\n",
    "The indices are in [0, numLabels),ordered by label frequencies, so the most frequent label gets index 0.\n",
    "\n",
    "In our case, the label column (Category) will be encoded to label indices, from 0 to 32; the most frequent label (LARCENY/THEFT) will be indexed as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|      Category|            Descript|               words|            filtered|            features|label|\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|      WARRANTS|      WARRANT ARREST|   [warrant, arrest]|   [warrant, arrest]|(809,[17,32],[1.0...|  7.0|\n",
      "|OTHER OFFENSES|TRAFFIC VIOLATION...|[traffic, violati...|[traffic, violati...|(809,[11,17,35],[...|  1.0|\n",
      "|OTHER OFFENSES|TRAFFIC VIOLATION...|[traffic, violati...|[traffic, violati...|(809,[11,17,35],[...|  1.0|\n",
      "| LARCENY/THEFT|GRAND THEFT FROM ...|[grand, theft, fr...|[grand, theft, fr...|(809,[0,2,3,4,6],...|  0.0|\n",
      "| LARCENY/THEFT|GRAND THEFT FROM ...|[grand, theft, fr...|[grand, theft, fr...|(809,[0,2,3,4,6],...|  0.0|\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## libs import\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"Category\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition Training & Test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 615087\n",
      "Test Dataset Count: 262962\n"
     ]
    }
   ],
   "source": [
    "# set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression using Count Vector Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "|                      Descript|     Category|                   probability|label|prediction|\n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.871761743000378,0.021247...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.871761743000378,0.021247...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.871761743000378,0.021247...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.871761743000378,0.021247...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.871761743000378,0.021247...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.871761743000378,0.021247...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.871761743000378,0.021247...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, SERIA...|LARCENY/THEFT|[0.8717609103059799,0.02124...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, SERIA...|LARCENY/THEFT|[0.8717609103059799,0.02124...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, SERIA...|LARCENY/THEFT|[0.8717609103059799,0.02124...|  0.0|       0.0|\n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Linear regression Model\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0) ## Calling Logistic regression\n",
    "lrModel = lr.fit(trainingData) ## Fit The data on it\n",
    "predictions = lrModel.transform(testData) ## Predict using the model\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evalution \n",
    "each time when we want to evaluate the models we will use MulticlassClassificationEvaluator for mor info [link_to_visit](https://spark.apache.org/docs/2.3.0/mllib-evaluation-metrics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9722407474605252"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Libs import\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator \n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression using TF-IDF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "|                      Descript|     Category|                   probability|label|prediction|\n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8717617430003752,0.02124...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8717617430003752,0.02124...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8717617430003752,0.02124...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8717617430003752,0.02124...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8717617430003752,0.02124...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8717617430003752,0.02124...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8717617430003752,0.02124...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, SERIA...|LARCENY/THEFT|[0.8717609103059791,0.02124...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, SERIA...|LARCENY/THEFT|[0.8717609103059791,0.02124...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, SERIA...|LARCENY/THEFT|[0.8717609103059791,0.02124...|  0.0|       0.0|\n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx])\n",
    "\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions = lrModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9722407474605252"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Model Evalution \n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is excellent! 👍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------+------------------------------+-----+----------+\n",
      "|                    Descript|     Category|                   probability|label|prediction|\n",
      "+----------------------------+-------------+------------------------------+-----+----------+\n",
      "|GRAND THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.9400681504940949,0.00879...|  0.0|       0.0|\n",
      "|GRAND THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.9400681504940949,0.00879...|  0.0|       0.0|\n",
      "|GRAND THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.9400681504940949,0.00879...|  0.0|       0.0|\n",
      "|GRAND THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.9400681504940949,0.00879...|  0.0|       0.0|\n",
      "|GRAND THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.9400681504940949,0.00879...|  0.0|       0.0|\n",
      "|GRAND THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.9400681504940949,0.00879...|  0.0|       0.0|\n",
      "|GRAND THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.9400681504940949,0.00879...|  0.0|       0.0|\n",
      "|GRAND THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.9400681504940949,0.00879...|  0.0|       0.0|\n",
      "|GRAND THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.9400681504940949,0.00879...|  0.0|       0.0|\n",
      "|GRAND THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.9400681504940949,0.00879...|  0.0|       0.0|\n",
      "+----------------------------+-------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 15, \\\n",
    "                            maxBins = 50)\n",
    "# Train model with Training Data\n",
    "rfModel = rf.fit(trainingData)\n",
    "predictions = rfModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8714910993501884"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Model Evalution \n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The accuracy is not bad, it's still to be ameliorated !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let’s now try cross-validation to tune our hyper parameters, and we will only tune the count vectors Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9918194984396738"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0) ### Logistic Regression \n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "             .build())# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5) ## number of Folds created\n",
    "cvModel = cv.fit(trainingData)\n",
    "\n",
    "predictions = cvModel.transform(testData)\n",
    "# Evaluate best model\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Random forest is a very good, robust and versatile method, however it’s no mystery that for high-dimensional sparse data it’s not a best choice.\n",
    "It is obvious that Logistic Regression will be our model in this experiment, with cross validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-3 Spark-2.4.4 Local (All cores)",
   "language": "python",
   "name": "datalab-python-3-spark-2.4.4-local-all_cores"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
